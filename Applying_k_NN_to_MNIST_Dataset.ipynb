{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Applying k-NN to MNIST Dataset",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohith1-p/Rohith1-p/blob/main/Applying_k_NN_to_MNIST_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98c17213-ead9-401b-ab44-7ac757463452"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhCQhICpnDAc"
      },
      "source": [
        "## Downloading MNIST Train and Test Datasets  \n",
        " \n",
        "* **Proceed to further steps only after executing the cells in this section**.\n",
        "* The variables from these steps are used in some of the sample test cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "af2b3cb8-8391-471c-9271-03ad0901be20"
      },
      "source": [
        "# Downloading the datasets using wget\n",
        "!wget https://nkb-backend-otg-media-static.s3.ap-south-1.amazonaws.com/otg_prod/media/Tech_4.0/AI_ML/Datasets/mnist_train.csv\n",
        "!wget https://nkb-backend-otg-media-static.s3.ap-south-1.amazonaws.com/otg_prod/media/Tech_4.0/AI_ML/Datasets/mnist_test.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGbpsb811xRu"
      },
      "source": [
        "**NOTE:** Executing the below cell might take some time (1-2 min) as the original MNIST dataset is large."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47d30efd-945e-4302-a9c2-670178356231"
      },
      "source": [
        "train_file_name = \"mnist_train.csv\"\n",
        "train_data = np.genfromtxt(train_file_name, delimiter=',', dtype=np.uint16)\n",
        "print(f\"Shape of the train_data in {train_file_name} is: {train_data.shape} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dcc216c-2afc-49fd-aad0-a6f78e995a6b"
      },
      "source": [
        "MNIST_train_Y = train_data[:, 0].reshape(-1, 1)\n",
        "MNIST_train_X = train_data[:, 1:]\n",
        "\n",
        "print(f\"Shape of X: {MNIST_train_X.shape} \\n\")\n",
        "print(f\"Shape of Y: {MNIST_train_Y.shape} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvtdIABkKq_f"
      },
      "source": [
        "**NOTE:** We've used **`np.uint16`** to reduce the space taken by the input arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3m-4CRpiLW_"
      },
      "source": [
        "test_file_name = \"mnist_test.csv\"\n",
        "test_data = np.genfromtxt(test_file_name, delimiter=',', dtype=np.uint16)\n",
        "print(f\"Shape of the test_data in {test_file_name} is: {test_data.shape} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlGoXHaYiLD9"
      },
      "source": [
        "MNIST_test_Y = test_data[:, 0].reshape(-1, 1)\n",
        "MNIST_test_X = test_data[:, 1:]\n",
        "\n",
        "print(f\"Shape of X: {MNIST_test_Y.shape} \\n\")\n",
        "print(f\"Shape of Y: {MNIST_test_X.shape} \\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjmTy1hvlapF"
      },
      "source": [
        "### Sizes of Train and Test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJ-WI3nVkKjU"
      },
      "source": [
        "print(f\"Size of train_X: {MNIST_train_X.nbytes}\")\n",
        "print(f\"Size of train_Y: {MNIST_train_Y.nbytes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-boAKvZkKRJ"
      },
      "source": [
        "print(f\"Size of test_X: {MNIST_test_X.nbytes}\")\n",
        "print(f\"Size of test_Y: {MNIST_test_Y.nbytes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXk8eQ7Kloow"
      },
      "source": [
        "## k-NN Algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8WN-hVHmsdR"
      },
      "source": [
        "### Split Train and Validation Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AabMkisql8HQ"
      },
      "source": [
        "import math\n",
        "def shuffle(X, Y):\n",
        "  np.random.seed(2) \n",
        "  indices = np.random.permutation(X.shape[0])\n",
        "  shuffled_X = X[indices]\n",
        "  shuffled_Y = Y[indices]\n",
        "  return shuffled_X, shuffled_Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjHZreJiKZST"
      },
      "source": [
        "Using a fixed validation set size instead of percentage\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QLn5QbBmRXf"
      },
      "source": [
        "  inputs, labels = shuffle(MNIST_train_X, MNIST_train_Y)\n",
        "  train_length = 59000\n",
        "  \n",
        "  train_inputs = inputs[:train_length]\n",
        "  train_labels = labels[:train_length]\n",
        "  validation_inputs = inputs[train_length:]\n",
        "  validation_labels = labels[train_length:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6xT-dgxnhgM"
      },
      "source": [
        "print(f\"Size of train_inputs: {train_inputs.nbytes}\")\n",
        "print(f\"Size of validation_inputs: {validation_inputs.nbytes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAXNBNqQmyyH"
      },
      "source": [
        "### Compute distances matrix\n",
        "We're computing the distances between all the validation inputs and training inputs beforehand, so that we need not compute them in every iteration of **`majority_based_knn`** function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-voDb2SLBzz"
      },
      "source": [
        "**NOTE:** We've used **`np.float32`** to reduce the space taken by the input arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o69sJZSem5iQ"
      },
      "source": [
        "train_count = train_inputs.shape[0]\n",
        "num_of_features = train_inputs.shape[1]\n",
        "validation_count = validation_inputs.shape[0]\n",
        "\n",
        "distances_matrix = np.zeros((validation_count, train_count), dtype=np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4XJpbnmntCd"
      },
      "source": [
        "print(f\"Size of distances_matrix: {distances_matrix.nbytes}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiQ_CftPyRgN"
      },
      "source": [
        "def Ln_norm_distances(train_X, test_x, n):\n",
        "    abs_diff = np.abs(train_X - test_x)  \n",
        "    summation = np.sum(np.power(abs_diff, n), axis=1)\n",
        "    ln_distances = np.power(summation, 1/n)\n",
        "    return ln_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyLyyXjqRFh3"
      },
      "source": [
        "We are computing L2 norm distances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTc5jF-cy385"
      },
      "source": [
        "n = 2\n",
        "import time\n",
        "for idx, each in enumerate(validation_inputs):\n",
        "  ln_distances = Ln_norm_distances(train_inputs, each, n)\n",
        "  distances_matrix[idx] = ln_distances"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHTzNKoW0wRG"
      },
      "source": [
        "### Majority Based k-NN\n",
        "\n",
        "We've updated the **`majority_based_knn`** function to use the `distances_matrix` which is computed beforehand."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50b2e6e6-800b-4242-8852-057d0e055969"
      },
      "source": [
        "def majority_based_knn(distances_matrix, train_Y, k):\n",
        "  unique_class_labels = np.unique(train_Y)\n",
        "  num_unique_classes = unique_class_labels.shape[0]\n",
        "\n",
        "  train_length = distances_matrix.shape[1]\n",
        "  test_length = distances_matrix.shape[0]\n",
        "\n",
        "  label_wise_counts = np.zeros((test_length, num_unique_classes))\n",
        "  label_wise_weights = np.zeros((test_length, num_unique_classes))\n",
        "\n",
        "  sorted_indices = np.argsort(distances_matrix, axis=1)\n",
        "\n",
        "  for test_idx in range(test_length):\n",
        "    # Getting k-Nearest Neighbors from distances matrix\n",
        "    test_distances = distances_matrix[test_idx]\n",
        "    sorted_test_indices = sorted_indices[test_idx]\n",
        "    kth_dist_repeat_count = 0\n",
        "    if train_length > k:\n",
        "      kth_neighbour_distance = test_distances[sorted_test_indices[k-1]] \n",
        "      kth_dist_repeat_count = np.count_nonzero(test_distances[k:] == kth_neighbour_distance)\n",
        "    indices_k = sorted_test_indices[:(k + kth_dist_repeat_count)]\n",
        "    distances_k = test_distances[indices_k]\n",
        "    labels_k = train_Y[indices_k]\n",
        "\n",
        "    for label_idx, each_label in enumerate(unique_class_labels):\n",
        "      label_weight = np.sum(np.where(labels_k == each_label, 1/distances_k, 0.0))\n",
        "      label_wise_weights[test_idx][label_idx] = label_weight\n",
        "      label_count = np.sum(np.where(labels_k == each_label, 1.0, 0.0))\n",
        "      label_wise_counts[test_idx][label_idx] = label_count\n",
        "  \n",
        "  output_labels = np.empty(test_length, dtype=int)\n",
        "\n",
        "  sorted_counts_indices = np.argsort(label_wise_counts, axis=1)\n",
        "  for test_idx, test_indices in enumerate(sorted_counts_indices):\n",
        "    highest_count = label_wise_counts[test_idx][test_indices[num_unique_classes-1]]\n",
        "    highest_label_repeat = np.count_nonzero(label_wise_counts[test_idx] == highest_count)\n",
        "    \n",
        "    no_voting_tie = (highest_label_repeat==1)\n",
        "    if no_voting_tie:\n",
        "      output_labels[test_idx] = unique_class_labels[test_indices[num_unique_classes-1]]\n",
        "    else:\n",
        "      tied_class_indices = test_indices[num_unique_classes-highest_label_repeat:]\n",
        "      tied_class_weights = label_wise_weights[test_idx][tied_class_indices]\n",
        "      max_weight_idx = np.argmax(tied_class_weights)\n",
        "      max_idx = tied_class_indices[max_weight_idx]\n",
        "      output_labels[test_idx] = unique_class_labels[max_idx]\n",
        "\n",
        "  return output_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b65d04ea-00b8-472a-a17c-ccf7e5d7c5bd"
      },
      "source": [
        "def calculate_accuracy(predicted_labels, actual_labels):\n",
        "    correctly_predicted_count = np.count_nonzero(predicted_labels == actual_labels)\n",
        "    accuracy = float(correctly_predicted_count)/predicted_labels.size\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_3pj7pzN4pi"
      },
      "source": [
        "**NOTE:** **`distances_matrix`** contains 'L2' distances between training and validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uhZdZ9q01Xc"
      },
      "source": [
        "k = 20\n",
        "output_labels = majority_based_knn(distances_matrix, train_labels, k)\n",
        "accuracy = calculate_accuracy(output_labels.flatten(), validation_labels.flatten())\n",
        "print(f\"accuracy for (k, n) {k , n} is : {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD6b2bQMOq1N"
      },
      "source": [
        "### Best `(k, n)` pair\n",
        "\n",
        "Compute the best `(k, n)` pair based on the above changes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTQiWF7MRcWB"
      },
      "source": [
        "# ToDo"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}